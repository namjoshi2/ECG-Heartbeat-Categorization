{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kaf0thJYWWx",
        "outputId": "431521a6-b566-483f-aa68-0de7b4499900"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"shayanfazeli/heartbeat\")\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKWsS6d0Ys7U",
        "outputId": "e6a98b8b-f623-4640-9f64-ed7607b18871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'heartbeat' dataset.\n",
            "Path to dataset files: /kaggle/input/heartbeat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ECGDataset(Dataset):\n",
        "    def __init__(self, csv_path, mean=None, std=None, fit_stats=False):\n",
        "        df = pd.read_csv(csv_path, header=None)\n",
        "        data = df.values\n",
        "        X = data[:, :-1].astype(np.float32)   # 187 features\n",
        "        y = data[:, -1].astype(np.int64)      # labels 0..4\n",
        "\n",
        "        if fit_stats:\n",
        "            # compute normalization stats from training data only\n",
        "            self.mean = X.mean(axis=0, keepdims=True)\n",
        "            self.std = X.std(axis=0, keepdims=True) + 1e-8\n",
        "        else:\n",
        "            self.mean = mean\n",
        "            self.std = std\n",
        "\n",
        "        X = (X - self.mean) / self.std\n",
        "\n",
        "        self.X = torch.from_numpy(X)\n",
        "        self.y = torch.from_numpy(y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n"
      ],
      "metadata": {
        "id": "mOOobBAAYwqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_csv = os.path.join(path, \"mitbih_train.csv\")\n",
        "test_csv  = os.path.join(path, \"mitbih_test.csv\")\n",
        "\n",
        "tmp_train = pd.read_csv(train_csv, header=None).values\n",
        "train_mean = tmp_train[:, :-1].astype(np.float32).mean(axis=0, keepdims=True)\n",
        "train_std = tmp_train[:, :-1].astype(np.float32).std(axis=0, keepdims=True) + 1e-8\n",
        "\n",
        "train_dataset = ECGDataset(train_csv, mean=train_mean, std=train_std, fit_stats=False)\n",
        "test_dataset  = ECGDataset(test_csv,  mean=train_mean, std=train_std, fit_stats=False)\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "\n",
        "num_features = train_dataset.X.shape[1]   # should be 187\n",
        "num_classes = len(torch.unique(train_dataset.y))  # should be 5\n"
      ],
      "metadata": {
        "id": "5p7TE46QY1OI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Addressing class imbalance using weights. i.e assigning highers weights to minority classes, and vice versa\n",
        "labels_np = train_dataset.y.numpy()\n",
        "class_counts = np.bincount(labels_np)\n",
        "class_weights = 1.0 / (class_counts + 1e-8)\n",
        "class_weights = class_weights * (len(class_counts) / class_weights.sum())  # normalize a bit\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
        "print(\"Class counts:\", class_counts)\n",
        "print(\"Class weights:\", class_weights)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HffCmYguY6Bp",
        "outputId": "39430ec2-5072-4d17-d121-3ee9c3fe8191"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class counts: [72471  2223  5788   641  6431]\n",
            "Class weights: [0.02933416 0.95630948 0.36729025 3.31649917 0.33056694]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#FNN Model\n",
        "class ECGFNN(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(ECGFNN, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(64, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model = ECGFNN(num_features, num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)  # you can remove \"weight=...\" at first\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
      ],
      "metadata": {
        "id": "Aa3Jn3A9Y9c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================== Training & Evaluation Loops ==================\n",
        "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for X_batch, y_batch in loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * X_batch.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct += (predicted == y_batch).sum().item()\n",
        "        total += y_batch.size(0)\n",
        "\n",
        "    avg_loss = running_loss / total\n",
        "    acc = correct / total\n",
        "    return avg_loss, acc\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    for X_batch, y_batch in loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "\n",
        "        running_loss += loss.item() * X_batch.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct += (predicted == y_batch).sum().item()\n",
        "        total += y_batch.size(0)\n",
        "\n",
        "        all_labels.append(y_batch.cpu().numpy())\n",
        "        all_preds.append(predicted.cpu().numpy())\n",
        "\n",
        "    avg_loss = running_loss / total\n",
        "    acc = correct / total\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    return avg_loss, acc, all_labels, all_preds\n"
      ],
      "metadata": {
        "id": "0lS6ffsrZJw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # ----- FNN -----\n",
        "    train_loss_fnn, train_acc_fnn = train_one_epoch(\n",
        "        model, train_loader, criterion, optimizer, device\n",
        "    )\n",
        "    val_loss_fnn, val_acc_fnn, _, _ = evaluate(\n",
        "        model, test_loader, criterion, device\n",
        "    )\n",
        "\n",
        "    print(f\"Epoch {epoch:02d}\")\n",
        "    print(f\"  [FNN] Train Loss: {train_loss_fnn:.4f}, Train Acc: {train_acc_fnn:.4f} | \"\n",
        "          f\"Test Loss: {val_loss_fnn:.4f}, Test Acc: {val_acc_fnn:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcpxyOh5Dn_E",
        "outputId": "e6efca02-2dcf-4d4a-9467-268b78da303c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01\n",
            "  [FNN] Train Loss: 0.7134, Train Acc: 0.6276 | Test Loss: 0.5716, Test Acc: 0.8155\n",
            "Epoch 02\n",
            "  [FNN] Train Loss: 0.4893, Train Acc: 0.7776 | Test Loss: 0.5020, Test Acc: 0.8480\n",
            "Epoch 03\n",
            "  [FNN] Train Loss: 0.4270, Train Acc: 0.8072 | Test Loss: 0.3411, Test Acc: 0.8953\n",
            "Epoch 04\n",
            "  [FNN] Train Loss: 0.3956, Train Acc: 0.8324 | Test Loss: 0.3377, Test Acc: 0.8973\n",
            "Epoch 05\n",
            "  [FNN] Train Loss: 0.3702, Train Acc: 0.8372 | Test Loss: 0.4859, Test Acc: 0.8325\n",
            "Epoch 06\n",
            "  [FNN] Train Loss: 0.3416, Train Acc: 0.8458 | Test Loss: 0.3239, Test Acc: 0.9000\n",
            "Epoch 07\n",
            "  [FNN] Train Loss: 0.3294, Train Acc: 0.8538 | Test Loss: 0.3739, Test Acc: 0.8716\n",
            "Epoch 08\n",
            "  [FNN] Train Loss: 0.3204, Train Acc: 0.8602 | Test Loss: 0.3393, Test Acc: 0.8864\n",
            "Epoch 09\n",
            "  [FNN] Train Loss: 0.3008, Train Acc: 0.8639 | Test Loss: 0.3732, Test Acc: 0.8791\n",
            "Epoch 10\n",
            "  [FNN] Train Loss: 0.3027, Train Acc: 0.8657 | Test Loss: 0.3427, Test Acc: 0.8965\n",
            "Epoch 11\n",
            "  [FNN] Train Loss: 0.2908, Train Acc: 0.8713 | Test Loss: 0.3118, Test Acc: 0.9024\n",
            "Epoch 12\n",
            "  [FNN] Train Loss: 0.2838, Train Acc: 0.8748 | Test Loss: 0.3834, Test Acc: 0.8627\n",
            "Epoch 13\n",
            "  [FNN] Train Loss: 0.2629, Train Acc: 0.8787 | Test Loss: 0.2856, Test Acc: 0.9014\n",
            "Epoch 14\n",
            "  [FNN] Train Loss: 0.2607, Train Acc: 0.8763 | Test Loss: 0.2673, Test Acc: 0.9043\n",
            "Epoch 15\n",
            "  [FNN] Train Loss: 0.2528, Train Acc: 0.8778 | Test Loss: 0.2541, Test Acc: 0.9083\n",
            "Epoch 16\n",
            "  [FNN] Train Loss: 0.2896, Train Acc: 0.8692 | Test Loss: 0.3191, Test Acc: 0.8840\n",
            "Epoch 17\n",
            "  [FNN] Train Loss: 0.2541, Train Acc: 0.8812 | Test Loss: 0.2782, Test Acc: 0.9111\n",
            "Epoch 18\n",
            "  [FNN] Train Loss: 0.2422, Train Acc: 0.8802 | Test Loss: 0.2273, Test Acc: 0.9191\n",
            "Epoch 19\n",
            "  [FNN] Train Loss: 0.2447, Train Acc: 0.8838 | Test Loss: 0.2628, Test Acc: 0.9109\n",
            "Epoch 20\n",
            "  [FNN] Train Loss: 0.2463, Train Acc: 0.8801 | Test Loss: 0.2883, Test Acc: 0.9005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CNN Model\n",
        "class ECGCNN1D(nn.Module):\n",
        "    def __init__(self, num_classes, input_length=187):\n",
        "        super(ECGCNN1D, self).__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv1d(1, 32, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2),   # 187 -> 93\n",
        "\n",
        "            nn.Conv1d(32, 64, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2),   # 93 -> 46\n",
        "\n",
        "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),                     # length = 46\n",
        "        )\n",
        "\n",
        "        # ðŸ”¥ MPS-safe global pooling\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(23)    # always works (46 â†’ 1)\n",
        "\n",
        "        # 128 channels * 1 time step = 128 features\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2944, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # (batch, 187) â†’ (batch, 1, 187)\n",
        "        x = self.features(x)\n",
        "        x = self.global_pool(x)  # (batch, 128, 1)\n",
        "        return self.classifier(x)  # (batch, num_classes)\n",
        "\n",
        "model2 = ECGCNN1D(num_classes=num_classes).to(device)\n",
        "\n",
        "criterion2 = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "\n",
        "optimizer2 = torch.optim.Adam(model2.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "CrImEORgZCth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # ----- CNN -----\n",
        "    train_loss_cnn, train_acc_cnn = train_one_epoch(\n",
        "        model2, train_loader, criterion2, optimizer2, device\n",
        "    )\n",
        "    val_loss_cnn, val_acc_cnn, _, _ = evaluate(\n",
        "        model2, test_loader, criterion2, device\n",
        "    )\n",
        "\n",
        "    print(f\"Epoch {epoch:02d}\")\n",
        "    print(f\"  [CNN] Train Loss: {train_loss_cnn:.4f}, Train Acc: {train_acc_cnn:.4f} | \"\n",
        "          f\"Test Loss: {val_loss_cnn:.4f}, Test Acc: {val_acc_cnn:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWF4SnNMZPoI",
        "outputId": "1650e4b7-65c5-46fc-bbcc-12db5ac9f2ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01\n",
            "  [CNN] Train Loss: 0.4984, Train Acc: 0.7824 | Test Loss: 0.5525, Test Acc: 0.8270\n",
            "Epoch 02\n",
            "  [CNN] Train Loss: 0.3202, Train Acc: 0.8619 | Test Loss: 0.2211, Test Acc: 0.9374\n",
            "Epoch 03\n",
            "  [CNN] Train Loss: 0.2804, Train Acc: 0.8832 | Test Loss: 0.2457, Test Acc: 0.9216\n",
            "Epoch 04\n",
            "  [CNN] Train Loss: 0.2315, Train Acc: 0.8994 | Test Loss: 0.2820, Test Acc: 0.9051\n",
            "Epoch 05\n",
            "  [CNN] Train Loss: 0.2061, Train Acc: 0.9102 | Test Loss: 0.1375, Test Acc: 0.9587\n",
            "Epoch 06\n",
            "  [CNN] Train Loss: 0.1966, Train Acc: 0.9165 | Test Loss: 0.2215, Test Acc: 0.9240\n",
            "Epoch 07\n",
            "  [CNN] Train Loss: 0.1774, Train Acc: 0.9194 | Test Loss: 0.1921, Test Acc: 0.9356\n",
            "Epoch 08\n",
            "  [CNN] Train Loss: 0.1709, Train Acc: 0.9256 | Test Loss: 0.1527, Test Acc: 0.9530\n",
            "Epoch 09\n",
            "  [CNN] Train Loss: 0.1478, Train Acc: 0.9298 | Test Loss: 0.1480, Test Acc: 0.9514\n",
            "Epoch 10\n",
            "  [CNN] Train Loss: 0.1368, Train Acc: 0.9347 | Test Loss: 0.1776, Test Acc: 0.9437\n",
            "Epoch 11\n",
            "  [CNN] Train Loss: 0.1290, Train Acc: 0.9378 | Test Loss: 0.1647, Test Acc: 0.9497\n",
            "Epoch 12\n",
            "  [CNN] Train Loss: 0.1220, Train Acc: 0.9395 | Test Loss: 0.1283, Test Acc: 0.9598\n",
            "Epoch 13\n",
            "  [CNN] Train Loss: 0.1211, Train Acc: 0.9412 | Test Loss: 0.1206, Test Acc: 0.9631\n",
            "Epoch 14\n",
            "  [CNN] Train Loss: 0.1140, Train Acc: 0.9450 | Test Loss: 0.1320, Test Acc: 0.9595\n",
            "Epoch 15\n",
            "  [CNN] Train Loss: 0.1009, Train Acc: 0.9488 | Test Loss: 0.1225, Test Acc: 0.9631\n",
            "Epoch 16\n",
            "  [CNN] Train Loss: 0.1002, Train Acc: 0.9499 | Test Loss: 0.1372, Test Acc: 0.9552\n",
            "Epoch 17\n",
            "  [CNN] Train Loss: 0.0925, Train Acc: 0.9527 | Test Loss: 0.1445, Test Acc: 0.9564\n",
            "Epoch 18\n",
            "  [CNN] Train Loss: 0.0986, Train Acc: 0.9489 | Test Loss: 0.1020, Test Acc: 0.9716\n",
            "Epoch 19\n",
            "  [CNN] Train Loss: 0.0863, Train Acc: 0.9556 | Test Loss: 0.1383, Test Acc: 0.9593\n",
            "Epoch 20\n",
            "  [CNN] Train Loss: 0.0809, Train Acc: 0.9549 | Test Loss: 0.1186, Test Acc: 0.9625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================== Final Evaluation: FNN ==================\n",
        "fnn_test_loss, fnn_test_acc, fnn_y_true, fnn_y_pred = evaluate(\n",
        "    model, test_loader, criterion, device\n",
        ")\n",
        "\n",
        "print(\"\\n===== FNN Results =====\")\n",
        "print(\"Final Test Loss (FNN):\", fnn_test_loss)\n",
        "print(\"Final Test Accuracy (FNN):\", fnn_test_acc)\n",
        "\n",
        "print(\"\\n[FNN] Classification Report:\")\n",
        "print(classification_report(fnn_y_true, fnn_y_pred, digits=4))\n",
        "\n",
        "print(\"[FNN] Confusion Matrix:\")\n",
        "print(confusion_matrix(fnn_y_true, fnn_y_pred))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8X4ze15tZTto",
        "outputId": "8cec7b45-3fb4-47a4-c882-f72e480a8ca2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== FNN Results =====\n",
            "Final Test Loss (FNN): 0.2882586213830357\n",
            "Final Test Accuracy (FNN): 0.9005116024118399\n",
            "\n",
            "[FNN] Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9935    0.8922    0.9401     18118\n",
            "           1     0.2729    0.8453    0.4126       556\n",
            "           2     0.8220    0.9344    0.8746      1448\n",
            "           3     0.2668    0.9074    0.4123       162\n",
            "           4     0.9272    0.9826    0.9541      1608\n",
            "\n",
            "    accuracy                         0.9005     21892\n",
            "   macro avg     0.6565    0.9124    0.7188     21892\n",
            "weighted avg     0.9536    0.9005    0.9195     21892\n",
            "\n",
            "[FNN] Confusion Matrix:\n",
            "[[16164  1225   262   352   115]\n",
            " [   65   470    15     2     4]\n",
            " [   29    13  1353    48     5]\n",
            " [    5     3     7   147     0]\n",
            " [    6    11     9     2  1580]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================== Final Evaluation: CNN ==================\n",
        "cnn_test_loss, cnn_test_acc, cnn_y_true, cnn_y_pred = evaluate(\n",
        "    model2, test_loader, criterion2, device\n",
        ")\n",
        "\n",
        "print(\"\\n===== CNN Results =====\")\n",
        "print(\"Final Test Loss (CNN):\", cnn_test_loss)\n",
        "print(\"Final Test Accuracy (CNN):\", cnn_test_acc)\n",
        "\n",
        "print(\"\\n[CNN] Classification Report:\")\n",
        "print(classification_report(cnn_y_true, cnn_y_pred, digits=4))\n",
        "\n",
        "print(\"[CNN] Confusion Matrix:\")\n",
        "print(confusion_matrix(cnn_y_true, cnn_y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f09EMmCVEg2V",
        "outputId": "f4f42cda-3e5c-4e68-c86d-a26c21b14902"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== CNN Results =====\n",
            "Final Test Loss (CNN): 0.11860908842214213\n",
            "Final Test Accuracy (CNN): 0.9624977160606615\n",
            "\n",
            "[CNN] Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9936    0.9662    0.9797     18118\n",
            "           1     0.6789    0.8291    0.7466       556\n",
            "           2     0.9102    0.9378    0.9238      1448\n",
            "           3     0.3202    0.9506    0.4790       162\n",
            "           4     0.9815    0.9900    0.9858      1608\n",
            "\n",
            "    accuracy                         0.9625     21892\n",
            "   macro avg     0.7769    0.9348    0.8230     21892\n",
            "weighted avg     0.9743    0.9625    0.9669     21892\n",
            "\n",
            "[CNN] Confusion Matrix:\n",
            "[[17506   210   116   262    24]\n",
            " [   78   461    12     4     1]\n",
            " [   18     7  1358    60     5]\n",
            " [    5     1     2   154     0]\n",
            " [   11     0     4     1  1592]]\n"
          ]
        }
      ]
    }
  ]
}