{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kaf0thJYWWx",
        "outputId": "5cfae4aa-e7f7-4194-f5cd-fa10db0f67d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"shayanfazeli/heartbeat\")\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKWsS6d0Ys7U",
        "outputId": "d16453e4-53bd-4fc0-99f0-7f57cd8705fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'heartbeat' dataset.\n",
            "Path to dataset files: /kaggle/input/heartbeat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ECGDataset(Dataset):\n",
        "    def __init__(self, csv_path, mean=None, std=None, fit_stats=False):\n",
        "        df = pd.read_csv(csv_path, header=None)\n",
        "        data = df.values\n",
        "        X = data[:, :-1].astype(np.float32)   # 187 features\n",
        "        y = data[:, -1].astype(np.int64)      # labels 0..4\n",
        "\n",
        "        if fit_stats:\n",
        "            # compute normalization stats from training data only\n",
        "            self.mean = X.mean(axis=0, keepdims=True)\n",
        "            self.std = X.std(axis=0, keepdims=True) + 1e-8\n",
        "        else:\n",
        "            self.mean = mean\n",
        "            self.std = std\n",
        "\n",
        "        X = (X - self.mean) / self.std\n",
        "\n",
        "        self.X = torch.from_numpy(X)\n",
        "        self.y = torch.from_numpy(y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n"
      ],
      "metadata": {
        "id": "mOOobBAAYwqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_csv = os.path.join(path, \"mitbih_train.csv\")\n",
        "test_csv  = os.path.join(path, \"mitbih_test.csv\")\n",
        "\n",
        "tmp_train = pd.read_csv(train_csv, header=None).values\n",
        "train_mean = tmp_train[:, :-1].astype(np.float32).mean(axis=0, keepdims=True)\n",
        "train_std = tmp_train[:, :-1].astype(np.float32).std(axis=0, keepdims=True) + 1e-8\n",
        "\n",
        "train_dataset = ECGDataset(train_csv, mean=train_mean, std=train_std, fit_stats=False)\n",
        "test_dataset  = ECGDataset(test_csv,  mean=train_mean, std=train_std, fit_stats=False)\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "\n",
        "num_features = train_dataset.X.shape[1]   # should be 187\n",
        "num_classes = len(torch.unique(train_dataset.y))  # should be 5\n"
      ],
      "metadata": {
        "id": "5p7TE46QY1OI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Addressing class imbalance using weights. i.e assigning highers weights to minority classes, and vice versa\n",
        "labels_np = train_dataset.y.numpy()\n",
        "class_counts = np.bincount(labels_np)\n",
        "class_weights = 1.0 / (class_counts + 1e-8)\n",
        "class_weights = class_weights * (len(class_counts) / class_weights.sum())  # normalize a bit\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
        "print(\"Class counts:\", class_counts)\n",
        "print(\"Class weights:\", class_weights)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HffCmYguY6Bp",
        "outputId": "b7aeab7a-f83f-47cc-a6f4-25ceb8bf89fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class counts: [72471  2223  5788   641  6431]\n",
            "Class weights: [0.02933416 0.95630948 0.36729025 3.31649917 0.33056694]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#FNN Model\n",
        "class ECGFNN(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(ECGFNN, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(64, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model = ECGFNN(num_features, num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)  # you can remove \"weight=...\" at first\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
      ],
      "metadata": {
        "id": "Aa3Jn3A9Y9c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================== Training & Evaluation Loops ==================\n",
        "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for X_batch, y_batch in loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * X_batch.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct += (predicted == y_batch).sum().item()\n",
        "        total += y_batch.size(0)\n",
        "\n",
        "    avg_loss = running_loss / total\n",
        "    acc = correct / total\n",
        "    return avg_loss, acc\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    for X_batch, y_batch in loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "\n",
        "        running_loss += loss.item() * X_batch.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct += (predicted == y_batch).sum().item()\n",
        "        total += y_batch.size(0)\n",
        "\n",
        "        all_labels.append(y_batch.cpu().numpy())\n",
        "        all_preds.append(predicted.cpu().numpy())\n",
        "\n",
        "    avg_loss = running_loss / total\n",
        "    acc = correct / total\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    return avg_loss, acc, all_labels, all_preds\n"
      ],
      "metadata": {
        "id": "0lS6ffsrZJw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # ----- FNN -----\n",
        "    train_loss_fnn, train_acc_fnn = train_one_epoch(\n",
        "        model, train_loader, criterion, optimizer, device\n",
        "    )\n",
        "    val_loss_fnn, val_acc_fnn, _, _ = evaluate(\n",
        "        model, test_loader, criterion, device\n",
        "    )\n",
        "\n",
        "    print(f\"Epoch {epoch:02d}\")\n",
        "    print(f\"  [FNN] Train Loss: {train_loss_fnn:.4f}, Train Acc: {train_acc_fnn:.4f} | \"\n",
        "          f\"Test Loss: {val_loss_fnn:.4f}, Test Acc: {val_acc_fnn:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcpxyOh5Dn_E",
        "outputId": "e6efca02-2dcf-4d4a-9467-268b78da303c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01\n",
            "  [FNN] Train Loss: 0.7134, Train Acc: 0.6276 | Test Loss: 0.5716, Test Acc: 0.8155\n",
            "Epoch 02\n",
            "  [FNN] Train Loss: 0.4893, Train Acc: 0.7776 | Test Loss: 0.5020, Test Acc: 0.8480\n",
            "Epoch 03\n",
            "  [FNN] Train Loss: 0.4270, Train Acc: 0.8072 | Test Loss: 0.3411, Test Acc: 0.8953\n",
            "Epoch 04\n",
            "  [FNN] Train Loss: 0.3956, Train Acc: 0.8324 | Test Loss: 0.3377, Test Acc: 0.8973\n",
            "Epoch 05\n",
            "  [FNN] Train Loss: 0.3702, Train Acc: 0.8372 | Test Loss: 0.4859, Test Acc: 0.8325\n",
            "Epoch 06\n",
            "  [FNN] Train Loss: 0.3416, Train Acc: 0.8458 | Test Loss: 0.3239, Test Acc: 0.9000\n",
            "Epoch 07\n",
            "  [FNN] Train Loss: 0.3294, Train Acc: 0.8538 | Test Loss: 0.3739, Test Acc: 0.8716\n",
            "Epoch 08\n",
            "  [FNN] Train Loss: 0.3204, Train Acc: 0.8602 | Test Loss: 0.3393, Test Acc: 0.8864\n",
            "Epoch 09\n",
            "  [FNN] Train Loss: 0.3008, Train Acc: 0.8639 | Test Loss: 0.3732, Test Acc: 0.8791\n",
            "Epoch 10\n",
            "  [FNN] Train Loss: 0.3027, Train Acc: 0.8657 | Test Loss: 0.3427, Test Acc: 0.8965\n",
            "Epoch 11\n",
            "  [FNN] Train Loss: 0.2908, Train Acc: 0.8713 | Test Loss: 0.3118, Test Acc: 0.9024\n",
            "Epoch 12\n",
            "  [FNN] Train Loss: 0.2838, Train Acc: 0.8748 | Test Loss: 0.3834, Test Acc: 0.8627\n",
            "Epoch 13\n",
            "  [FNN] Train Loss: 0.2629, Train Acc: 0.8787 | Test Loss: 0.2856, Test Acc: 0.9014\n",
            "Epoch 14\n",
            "  [FNN] Train Loss: 0.2607, Train Acc: 0.8763 | Test Loss: 0.2673, Test Acc: 0.9043\n",
            "Epoch 15\n",
            "  [FNN] Train Loss: 0.2528, Train Acc: 0.8778 | Test Loss: 0.2541, Test Acc: 0.9083\n",
            "Epoch 16\n",
            "  [FNN] Train Loss: 0.2896, Train Acc: 0.8692 | Test Loss: 0.3191, Test Acc: 0.8840\n",
            "Epoch 17\n",
            "  [FNN] Train Loss: 0.2541, Train Acc: 0.8812 | Test Loss: 0.2782, Test Acc: 0.9111\n",
            "Epoch 18\n",
            "  [FNN] Train Loss: 0.2422, Train Acc: 0.8802 | Test Loss: 0.2273, Test Acc: 0.9191\n",
            "Epoch 19\n",
            "  [FNN] Train Loss: 0.2447, Train Acc: 0.8838 | Test Loss: 0.2628, Test Acc: 0.9109\n",
            "Epoch 20\n",
            "  [FNN] Train Loss: 0.2463, Train Acc: 0.8801 | Test Loss: 0.2883, Test Acc: 0.9005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CNN Model\n",
        "class ECGCNN1D(nn.Module):\n",
        "    def __init__(self, num_classes, input_length=187):\n",
        "        super(ECGCNN1D, self).__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv1d(1, 32, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2),   # 187 -> 93\n",
        "\n",
        "            nn.Conv1d(32, 64, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2),   # 93 -> 46\n",
        "\n",
        "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),                     # length = 46\n",
        "        )\n",
        "\n",
        "        # ðŸ”¥ MPS-safe global pooling\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(23)    # always works (46 â†’ 1)\n",
        "\n",
        "        # 128 channels * 1 time step = 128 features\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2944, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # (batch, 187) â†’ (batch, 1, 187)\n",
        "        x = self.features(x)\n",
        "        x = self.global_pool(x)  # (batch, 128, 1)\n",
        "        return self.classifier(x)  # (batch, num_classes)\n",
        "\n",
        "model2 = ECGCNN1D(num_classes=num_classes).to(device)\n",
        "\n",
        "criterion2 = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "\n",
        "optimizer2 = torch.optim.Adam(model2.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "CrImEORgZCth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # ----- CNN -----\n",
        "    train_loss_cnn, train_acc_cnn = train_one_epoch(\n",
        "        model2, train_loader, criterion2, optimizer2, device\n",
        "    )\n",
        "    val_loss_cnn, val_acc_cnn, _, _ = evaluate(\n",
        "        model2, test_loader, criterion2, device\n",
        "    )\n",
        "\n",
        "    print(f\"Epoch {epoch:02d}\")\n",
        "    print(f\"  [CNN] Train Loss: {train_loss_cnn:.4f}, Train Acc: {train_acc_cnn:.4f} | \"\n",
        "          f\"Test Loss: {val_loss_cnn:.4f}, Test Acc: {val_acc_cnn:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWF4SnNMZPoI",
        "outputId": "1650e4b7-65c5-46fc-bbcc-12db5ac9f2ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01\n",
            "  [CNN] Train Loss: 0.4984, Train Acc: 0.7824 | Test Loss: 0.5525, Test Acc: 0.8270\n",
            "Epoch 02\n",
            "  [CNN] Train Loss: 0.3202, Train Acc: 0.8619 | Test Loss: 0.2211, Test Acc: 0.9374\n",
            "Epoch 03\n",
            "  [CNN] Train Loss: 0.2804, Train Acc: 0.8832 | Test Loss: 0.2457, Test Acc: 0.9216\n",
            "Epoch 04\n",
            "  [CNN] Train Loss: 0.2315, Train Acc: 0.8994 | Test Loss: 0.2820, Test Acc: 0.9051\n",
            "Epoch 05\n",
            "  [CNN] Train Loss: 0.2061, Train Acc: 0.9102 | Test Loss: 0.1375, Test Acc: 0.9587\n",
            "Epoch 06\n",
            "  [CNN] Train Loss: 0.1966, Train Acc: 0.9165 | Test Loss: 0.2215, Test Acc: 0.9240\n",
            "Epoch 07\n",
            "  [CNN] Train Loss: 0.1774, Train Acc: 0.9194 | Test Loss: 0.1921, Test Acc: 0.9356\n",
            "Epoch 08\n",
            "  [CNN] Train Loss: 0.1709, Train Acc: 0.9256 | Test Loss: 0.1527, Test Acc: 0.9530\n",
            "Epoch 09\n",
            "  [CNN] Train Loss: 0.1478, Train Acc: 0.9298 | Test Loss: 0.1480, Test Acc: 0.9514\n",
            "Epoch 10\n",
            "  [CNN] Train Loss: 0.1368, Train Acc: 0.9347 | Test Loss: 0.1776, Test Acc: 0.9437\n",
            "Epoch 11\n",
            "  [CNN] Train Loss: 0.1290, Train Acc: 0.9378 | Test Loss: 0.1647, Test Acc: 0.9497\n",
            "Epoch 12\n",
            "  [CNN] Train Loss: 0.1220, Train Acc: 0.9395 | Test Loss: 0.1283, Test Acc: 0.9598\n",
            "Epoch 13\n",
            "  [CNN] Train Loss: 0.1211, Train Acc: 0.9412 | Test Loss: 0.1206, Test Acc: 0.9631\n",
            "Epoch 14\n",
            "  [CNN] Train Loss: 0.1140, Train Acc: 0.9450 | Test Loss: 0.1320, Test Acc: 0.9595\n",
            "Epoch 15\n",
            "  [CNN] Train Loss: 0.1009, Train Acc: 0.9488 | Test Loss: 0.1225, Test Acc: 0.9631\n",
            "Epoch 16\n",
            "  [CNN] Train Loss: 0.1002, Train Acc: 0.9499 | Test Loss: 0.1372, Test Acc: 0.9552\n",
            "Epoch 17\n",
            "  [CNN] Train Loss: 0.0925, Train Acc: 0.9527 | Test Loss: 0.1445, Test Acc: 0.9564\n",
            "Epoch 18\n",
            "  [CNN] Train Loss: 0.0986, Train Acc: 0.9489 | Test Loss: 0.1020, Test Acc: 0.9716\n",
            "Epoch 19\n",
            "  [CNN] Train Loss: 0.0863, Train Acc: 0.9556 | Test Loss: 0.1383, Test Acc: 0.9593\n",
            "Epoch 20\n",
            "  [CNN] Train Loss: 0.0809, Train Acc: 0.9549 | Test Loss: 0.1186, Test Acc: 0.9625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================== Final Evaluation: FNN ==================\n",
        "fnn_test_loss, fnn_test_acc, fnn_y_true, fnn_y_pred = evaluate(\n",
        "    model, test_loader, criterion, device\n",
        ")\n",
        "\n",
        "print(\"\\n===== FNN Results =====\")\n",
        "print(\"Final Test Loss (FNN):\", fnn_test_loss)\n",
        "print(\"Final Test Accuracy (FNN):\", fnn_test_acc)\n",
        "\n",
        "print(\"\\n[FNN] Classification Report:\")\n",
        "print(classification_report(fnn_y_true, fnn_y_pred, digits=4))\n",
        "\n",
        "print(\"[FNN] Confusion Matrix:\")\n",
        "print(confusion_matrix(fnn_y_true, fnn_y_pred))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8X4ze15tZTto",
        "outputId": "8cec7b45-3fb4-47a4-c882-f72e480a8ca2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== FNN Results =====\n",
            "Final Test Loss (FNN): 0.2882586213830357\n",
            "Final Test Accuracy (FNN): 0.9005116024118399\n",
            "\n",
            "[FNN] Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9935    0.8922    0.9401     18118\n",
            "           1     0.2729    0.8453    0.4126       556\n",
            "           2     0.8220    0.9344    0.8746      1448\n",
            "           3     0.2668    0.9074    0.4123       162\n",
            "           4     0.9272    0.9826    0.9541      1608\n",
            "\n",
            "    accuracy                         0.9005     21892\n",
            "   macro avg     0.6565    0.9124    0.7188     21892\n",
            "weighted avg     0.9536    0.9005    0.9195     21892\n",
            "\n",
            "[FNN] Confusion Matrix:\n",
            "[[16164  1225   262   352   115]\n",
            " [   65   470    15     2     4]\n",
            " [   29    13  1353    48     5]\n",
            " [    5     3     7   147     0]\n",
            " [    6    11     9     2  1580]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================== Final Evaluation: CNN ==================\n",
        "cnn_test_loss, cnn_test_acc, cnn_y_true, cnn_y_pred = evaluate(\n",
        "    model2, test_loader, criterion2, device\n",
        ")\n",
        "\n",
        "print(\"\\n===== CNN Results =====\")\n",
        "print(\"Final Test Loss (CNN):\", cnn_test_loss)\n",
        "print(\"Final Test Accuracy (CNN):\", cnn_test_acc)\n",
        "\n",
        "print(\"\\n[CNN] Classification Report:\")\n",
        "print(classification_report(cnn_y_true, cnn_y_pred, digits=4))\n",
        "\n",
        "print(\"[CNN] Confusion Matrix:\")\n",
        "print(confusion_matrix(cnn_y_true, cnn_y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f09EMmCVEg2V",
        "outputId": "f4f42cda-3e5c-4e68-c86d-a26c21b14902"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== CNN Results =====\n",
            "Final Test Loss (CNN): 0.11860908842214213\n",
            "Final Test Accuracy (CNN): 0.9624977160606615\n",
            "\n",
            "[CNN] Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9936    0.9662    0.9797     18118\n",
            "           1     0.6789    0.8291    0.7466       556\n",
            "           2     0.9102    0.9378    0.9238      1448\n",
            "           3     0.3202    0.9506    0.4790       162\n",
            "           4     0.9815    0.9900    0.9858      1608\n",
            "\n",
            "    accuracy                         0.9625     21892\n",
            "   macro avg     0.7769    0.9348    0.8230     21892\n",
            "weighted avg     0.9743    0.9625    0.9669     21892\n",
            "\n",
            "[CNN] Confusion Matrix:\n",
            "[[17506   210   116   262    24]\n",
            " [   78   461    12     4     1]\n",
            " [   18     7  1358    60     5]\n",
            " [    5     1     2   154     0]\n",
            " [   11     0     4     1  1592]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Softer class weights for LSTM only =====\n",
        "labels_np = train_dataset.y.numpy()\n",
        "class_counts = np.bincount(labels_np)\n",
        "\n",
        "# Inverse frequency\n",
        "inv_freq = 1.0 / (class_counts + 1e-8)\n",
        "\n",
        "#  Soften it: sqrt instead of raw inverse\n",
        "class_weights_lstm = np.sqrt(inv_freq)\n",
        "\n",
        "# Normalize a bit so magnitudes are reasonable (optional)\n",
        "class_weights_lstm = class_weights_lstm * (len(class_counts) / class_weights_lstm.sum())\n",
        "\n",
        "class_weights_lstm_tensor = torch.tensor(\n",
        "    class_weights_lstm, dtype=torch.float32\n",
        ").to(device)\n",
        "\n",
        "print(\"Class counts:\", class_counts)\n",
        "print(\"LSTM class weights:\", class_weights_lstm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bt7GZQcCgm2U",
        "outputId": "5cbb15d9-aeed-461b-e7fd-2b1d7ec37383"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class counts: [72471  2223  5788   641  6431]\n",
            "LSTM class weights: [0.20628726 1.17783593 0.72994522 2.19343877 0.69249282]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ECGLSTM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes,\n",
        "        input_size=1,       # 1 feature per timestep (ECG amplitude)\n",
        "        hidden_size=64,\n",
        "        num_layers=2,       # DEEPER: 2-layer LSTM\n",
        "        bidirectional=True,\n",
        "        dropout=0.3         # a bit more dropout inside LSTM\n",
        "    ):\n",
        "        super(ECGLSTM, self).__init__()\n",
        "\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,          # input: (batch, seq_len, features)\n",
        "            bidirectional=bidirectional,\n",
        "            dropout=dropout if num_layers > 1 else 0.0,\n",
        "        )\n",
        "\n",
        "        dir_factor = 2 if bidirectional else 1\n",
        "\n",
        "        # We will use BOTH mean-pool and max-pool over time and concat them:\n",
        "        # out: (batch, seq_len, hidden * dir_factor)\n",
        "        # mean + max â†’ 2 * hidden * dir_factor\n",
        "        fc_in = hidden_size * dir_factor * 2\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(fc_in, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, 187) â†’ treat as sequence of length 187, 1 feature per step\n",
        "        x = x.unsqueeze(-1)                # (batch, 187, 1)\n",
        "\n",
        "        out, _ = self.lstm(x)              # (batch, seq_len=187, hidden*dirs)\n",
        "\n",
        "        # Temporal mean pooling\n",
        "        mean_pooled = out.mean(dim=1)      # (batch, hidden*dirs)\n",
        "\n",
        "        # Temporal max pooling\n",
        "        max_pooled, _ = out.max(dim=1)     # (batch, hidden*dirs)\n",
        "\n",
        "        # Concatenate mean + max\n",
        "        features = torch.cat([mean_pooled, max_pooled], dim=1)  # (batch, 2*hidden*dirs)\n",
        "\n",
        "        logits = self.classifier(features) # (batch, num_classes)\n",
        "        return logits\n",
        "\n",
        "num_classes = len(torch.unique(train_dataset.y))\n",
        "\n",
        "model3 = ECGLSTM(\n",
        "    num_classes=num_classes,\n",
        "    hidden_size=64,\n",
        "    num_layers=2,\n",
        "    bidirectional=True,\n",
        "    dropout=0.3\n",
        ").to(device)\n",
        "\n",
        "criterion3 = nn.CrossEntropyLoss(weight=class_weights_lstm_tensor)\n",
        "optimizer3 = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "avnf6lgdSsVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train_loss, train_acc = train_one_epoch(\n",
        "        model3, train_loader, criterion3, optimizer3, device\n",
        "    )\n",
        "    val_loss, val_acc, _, _ = evaluate(\n",
        "        model3, test_loader, criterion3, device\n",
        "    )\n",
        "\n",
        "    print(f\"[LSTM] Epoch {epoch:02d} | \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
        "          f\"Test Loss: {val_loss:.4f}, Test Acc: {val_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mj_J7Tz_SwfG",
        "outputId": "61cdbd7f-c7e3-4a02-e9c4-4d9ed6d399dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LSTM] Epoch 01 | Train Loss: 0.9415, Train Acc: 0.8488 | Test Loss: 0.3818, Test Acc: 0.9057\n",
            "[LSTM] Epoch 02 | Train Loss: 0.5130, Train Acc: 0.9131 | Test Loss: 0.2157, Test Acc: 0.9389\n",
            "[LSTM] Epoch 03 | Train Loss: 0.3871, Train Acc: 0.9321 | Test Loss: 0.1931, Test Acc: 0.9505\n",
            "[LSTM] Epoch 04 | Train Loss: 0.3416, Train Acc: 0.9396 | Test Loss: 0.1651, Test Acc: 0.9549\n",
            "[LSTM] Epoch 05 | Train Loss: 0.2996, Train Acc: 0.9490 | Test Loss: 0.1577, Test Acc: 0.9577\n",
            "[LSTM] Epoch 06 | Train Loss: 0.2766, Train Acc: 0.9529 | Test Loss: 0.1400, Test Acc: 0.9627\n",
            "[LSTM] Epoch 07 | Train Loss: 0.2622, Train Acc: 0.9537 | Test Loss: 0.1557, Test Acc: 0.9591\n",
            "[LSTM] Epoch 08 | Train Loss: 0.2508, Train Acc: 0.9540 | Test Loss: 0.1460, Test Acc: 0.9604\n",
            "[LSTM] Epoch 09 | Train Loss: 0.2346, Train Acc: 0.9582 | Test Loss: 0.1183, Test Acc: 0.9684\n",
            "[LSTM] Epoch 10 | Train Loss: 0.2109, Train Acc: 0.9616 | Test Loss: 0.1327, Test Acc: 0.9616\n",
            "[LSTM] Epoch 11 | Train Loss: 0.2105, Train Acc: 0.9612 | Test Loss: 0.1353, Test Acc: 0.9638\n",
            "[LSTM] Epoch 12 | Train Loss: 0.1976, Train Acc: 0.9631 | Test Loss: 0.1098, Test Acc: 0.9711\n",
            "[LSTM] Epoch 13 | Train Loss: 0.1857, Train Acc: 0.9661 | Test Loss: 0.1044, Test Acc: 0.9740\n",
            "[LSTM] Epoch 14 | Train Loss: 0.1863, Train Acc: 0.9649 | Test Loss: 0.1237, Test Acc: 0.9683\n",
            "[LSTM] Epoch 15 | Train Loss: 0.1844, Train Acc: 0.9645 | Test Loss: 0.1063, Test Acc: 0.9716\n",
            "[LSTM] Epoch 16 | Train Loss: 0.1757, Train Acc: 0.9661 | Test Loss: 0.1289, Test Acc: 0.9607\n",
            "[LSTM] Epoch 17 | Train Loss: 0.1762, Train Acc: 0.9660 | Test Loss: 0.1047, Test Acc: 0.9705\n",
            "[LSTM] Epoch 18 | Train Loss: 0.1549, Train Acc: 0.9694 | Test Loss: 0.0927, Test Acc: 0.9741\n",
            "[LSTM] Epoch 19 | Train Loss: 0.1460, Train Acc: 0.9705 | Test Loss: 0.0919, Test Acc: 0.9742\n",
            "[LSTM] Epoch 20 | Train Loss: 0.1464, Train Acc: 0.9697 | Test Loss: 0.0922, Test Acc: 0.9749\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc, y_true, y_pred = evaluate(\n",
        "    model, test_loader, criterion, device\n",
        ")\n",
        "\n",
        "print(\"\\n===== LSTM Results =====\")\n",
        "print(\"Final Test Loss:\", test_loss)\n",
        "print(\"Final Test Accuracy:\", test_acc)\n",
        "\n",
        "print(\"\\n[LSTM] Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, digits=4))\n",
        "\n",
        "print(\"[LSTM] Confusion Matrix:\")\n",
        "print(confusion_matrix(y_true, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7KQjDGjTVN0",
        "outputId": "b4122583-7b21-4ac5-a725-5da821cfe1a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== LSTM Results =====\n",
            "Final Test Loss: 0.09216902880129116\n",
            "Final Test Accuracy: 0.9749223460624886\n",
            "\n",
            "[LSTM] Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9890    0.9855    0.9873     18118\n",
            "           1     0.8316    0.7284    0.7766       556\n",
            "           2     0.9013    0.9523    0.9261      1448\n",
            "           3     0.6119    0.8272    0.7034       162\n",
            "           4     0.9794    0.9757    0.9776      1608\n",
            "\n",
            "    accuracy                         0.9749     21892\n",
            "   macro avg     0.8626    0.8938    0.8742     21892\n",
            "weighted avg     0.9757    0.9749    0.9751     21892\n",
            "\n",
            "[LSTM] Confusion Matrix:\n",
            "[[17856    76   115    45    26]\n",
            " [  135   405    11     2     3]\n",
            " [   27     3  1379    36     3]\n",
            " [   17     0    10   134     1]\n",
            " [   19     3    15     2  1569]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ECGCNNLSTM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes,\n",
        "        conv_channels=(32, 64, 128),\n",
        "        lstm_hidden=64,\n",
        "        lstm_layers=1,\n",
        "        bidirectional=True,\n",
        "        lstm_dropout=0.2,\n",
        "        fc_hidden=128\n",
        "    ):\n",
        "        super(ECGCNNLSTM, self).__init__()\n",
        "\n",
        "        # ---------- 1D CNN feature extractor ----------\n",
        "        # Input shape: (batch, 1, 187)\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(1, conv_channels[0], kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(conv_channels[0]),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2),            # 187 -> 93\n",
        "\n",
        "            nn.Conv1d(conv_channels[0], conv_channels[1], kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(conv_channels[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2),            # 93 -> 46\n",
        "\n",
        "            nn.Conv1d(conv_channels[1], conv_channels[2], kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(conv_channels[2]),\n",
        "            nn.ReLU(),                              # (batch, 128, 46)\n",
        "        )\n",
        "\n",
        "        # ---------- LSTM over CNN feature sequence ----------\n",
        "        self.bidirectional = bidirectional\n",
        "        self.lstm_input_size = conv_channels[2]     # 128\n",
        "        self.lstm_hidden = lstm_hidden\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.lstm_input_size,        # features per timestep\n",
        "            hidden_size=lstm_hidden,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True,                       # (batch, seq_len, feat)\n",
        "            bidirectional=bidirectional,\n",
        "            dropout=lstm_dropout if lstm_layers > 1 else 0.0,\n",
        "        )\n",
        "\n",
        "        dir_factor = 2 if bidirectional else 1\n",
        "\n",
        "        # we will use mean + max pooling over time â†’ 2 * hidden * dir_factor\n",
        "        fc_in = lstm_hidden * dir_factor * 2\n",
        "\n",
        "        # ---------- Classifier ----------\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(fc_in, fc_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(fc_hidden, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, 187)\n",
        "        x = x.unsqueeze(1)                 # (batch, 1, 187)\n",
        "\n",
        "        # ----- CNN -----\n",
        "        x = self.cnn(x)                    # (batch, C=128, L=46)\n",
        "\n",
        "        # Prepare for LSTM: (batch, seq_len, features)\n",
        "        x = x.transpose(1, 2)              # (batch, 46, 128)\n",
        "\n",
        "        # ----- LSTM -----\n",
        "        out, _ = self.lstm(x)              # (batch, 46, hidden*dirs)\n",
        "\n",
        "        # Global temporal pooling over the 46 steps\n",
        "        mean_pooled = out.mean(dim=1)      # (batch, hidden*dirs)\n",
        "        max_pooled, _ = out.max(dim=1)     # (batch, hidden*dirs)\n",
        "\n",
        "        features = torch.cat([mean_pooled, max_pooled], dim=1)  # (batch, 2*hidden*dirs)\n",
        "\n",
        "        # ----- Classifier -----\n",
        "        logits = self.classifier(features) # (batch, num_classes)\n",
        "        return logits\n",
        "\n",
        "\n",
        "num_classes = len(torch.unique(train_dataset.y))\n",
        "\n",
        "model_hybrid = ECGCNNLSTM(\n",
        "    num_classes=num_classes,\n",
        "    conv_channels=(32, 64, 128),\n",
        "    lstm_hidden=64,\n",
        "    lstm_layers=1,\n",
        "    bidirectional=True,\n",
        "    lstm_dropout=0.2,\n",
        "    fc_hidden=128\n",
        ").to(device)\n",
        "\n",
        "criterion_hybrid = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "optimizer_hybrid = torch.optim.Adam(model_hybrid.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "iUe-Oyjphxre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train_loss, train_acc = train_one_epoch(\n",
        "        model_hybrid, train_loader, criterion_hybrid, optimizer_hybrid, device\n",
        "    )\n",
        "    val_loss, val_acc, _, _ = evaluate(\n",
        "        model_hybrid, test_loader, criterion_hybrid, device\n",
        "    )\n",
        "\n",
        "    print(f\"[CNN-LSTM] Epoch {epoch:02d} | \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
        "          f\"Test Loss: {val_loss:.4f}, Test Acc: {val_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WN8ZQqISiNnR",
        "outputId": "98ec3603-f4c9-4779-ce44-10e5b9d16a04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNN-LSTM] Epoch 01 | Train Loss: 0.6325, Train Acc: 0.6778 | Test Loss: 0.3551, Test Acc: 0.9027\n",
            "[CNN-LSTM] Epoch 02 | Train Loss: 0.3548, Train Acc: 0.8422 | Test Loss: 0.3174, Test Acc: 0.9011\n",
            "[CNN-LSTM] Epoch 03 | Train Loss: 0.3014, Train Acc: 0.8686 | Test Loss: 0.2280, Test Acc: 0.9420\n",
            "[CNN-LSTM] Epoch 04 | Train Loss: 0.2656, Train Acc: 0.8806 | Test Loss: 0.3670, Test Acc: 0.8848\n",
            "[CNN-LSTM] Epoch 05 | Train Loss: 0.2451, Train Acc: 0.8893 | Test Loss: 0.3616, Test Acc: 0.8727\n",
            "[CNN-LSTM] Epoch 06 | Train Loss: 0.2337, Train Acc: 0.8960 | Test Loss: 0.1933, Test Acc: 0.9388\n",
            "[CNN-LSTM] Epoch 07 | Train Loss: 0.1939, Train Acc: 0.9088 | Test Loss: 0.1293, Test Acc: 0.9635\n",
            "[CNN-LSTM] Epoch 08 | Train Loss: 0.1837, Train Acc: 0.9146 | Test Loss: 0.2433, Test Acc: 0.9234\n",
            "[CNN-LSTM] Epoch 09 | Train Loss: 0.1970, Train Acc: 0.9113 | Test Loss: 0.2225, Test Acc: 0.9268\n",
            "[CNN-LSTM] Epoch 10 | Train Loss: 0.1730, Train Acc: 0.9199 | Test Loss: 0.3153, Test Acc: 0.8892\n",
            "[CNN-LSTM] Epoch 11 | Train Loss: 0.1731, Train Acc: 0.9187 | Test Loss: 0.2318, Test Acc: 0.9286\n",
            "[CNN-LSTM] Epoch 12 | Train Loss: 0.1660, Train Acc: 0.9205 | Test Loss: 0.1922, Test Acc: 0.9417\n",
            "[CNN-LSTM] Epoch 13 | Train Loss: 0.1443, Train Acc: 0.9286 | Test Loss: 0.1682, Test Acc: 0.9444\n",
            "[CNN-LSTM] Epoch 14 | Train Loss: 0.1399, Train Acc: 0.9266 | Test Loss: 0.2356, Test Acc: 0.9191\n",
            "[CNN-LSTM] Epoch 15 | Train Loss: 0.1232, Train Acc: 0.9362 | Test Loss: 0.1591, Test Acc: 0.9496\n",
            "[CNN-LSTM] Epoch 16 | Train Loss: 0.1177, Train Acc: 0.9396 | Test Loss: 0.4607, Test Acc: 0.8294\n",
            "[CNN-LSTM] Epoch 17 | Train Loss: 0.1520, Train Acc: 0.9167 | Test Loss: 0.2906, Test Acc: 0.8999\n",
            "[CNN-LSTM] Epoch 18 | Train Loss: 0.1212, Train Acc: 0.9334 | Test Loss: 0.1337, Test Acc: 0.9601\n",
            "[CNN-LSTM] Epoch 19 | Train Loss: 0.1165, Train Acc: 0.9366 | Test Loss: 0.1691, Test Acc: 0.9431\n",
            "[CNN-LSTM] Epoch 20 | Train Loss: 0.1447, Train Acc: 0.9227 | Test Loss: 0.1307, Test Acc: 0.9592\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc, y_true, y_pred = evaluate(\n",
        "    model_hybrid, test_loader, criterion_hybrid, device\n",
        ")\n",
        "\n",
        "print(\"\\n===== CNN-LSTM Results =====\")\n",
        "print(\"Final Test Loss:\", test_loss)\n",
        "print(\"Final Test Accuracy:\", test_acc)\n",
        "print(\"\\n[CNN-LSTM] Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, digits=4))\n",
        "print(\"[CNN-LSTM] Confusion Matrix:\")\n",
        "print(confusion_matrix(y_true, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTIsFz90iP4o",
        "outputId": "a3ada178-ccee-4d45-f97e-73b166c9425c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== CNN-LSTM Results =====\n",
            "Final Test Loss: 0.1307317849276555\n",
            "Final Test Accuracy: 0.959208843413119\n",
            "\n",
            "[CNN-LSTM] Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9931    0.9622    0.9774     18118\n",
            "           1     0.5066    0.8237    0.6274       556\n",
            "           2     0.9113    0.9434    0.9270      1448\n",
            "           3     0.4833    0.8951    0.6277       162\n",
            "           4     0.9767    0.9925    0.9846      1608\n",
            "\n",
            "    accuracy                         0.9592     21892\n",
            "   macro avg     0.7742    0.9234    0.8288     21892\n",
            "weighted avg     0.9704    0.9592    0.9631     21892\n",
            "\n",
            "[CNN-LSTM] Confusion Matrix:\n",
            "[[17434   438   108   108    30]\n",
            " [   75   458    12    10     1]\n",
            " [   33     6  1366    36     7]\n",
            " [    7     1     9   145     0]\n",
            " [    6     1     4     1  1596]]\n"
          ]
        }
      ]
    }
  ]
}